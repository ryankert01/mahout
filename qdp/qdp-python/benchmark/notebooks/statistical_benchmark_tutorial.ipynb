{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistical Benchmarking Tutorial for Apache Mahout QDP\n",
        "\n",
        "This notebook demonstrates how to use the new statistical benchmarking and visualization features added in Phases 1-3.\n",
        "\n",
        "## Features Covered\n",
        "\n",
        "1. **Phase 1**: Using benchmark utilities directly\n",
        "2. **Phase 2**: Running benchmarks in statistical mode\n",
        "3. **Phase 3**: Generating publication-ready visualizations\n",
        "4. **Phase 4**: Best practices for reproducible benchmarks\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python 3.9+\n",
        "- CUDA-capable GPU (for GPU benchmarks)\n",
        "- Mahout QDP installed with benchmark dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's import the necessary modules and check our environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Check CUDA availability\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Using Benchmark Utils Directly\n",
        "\n",
        "The benchmark_utils package provides low-level utilities for fair benchmarking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add benchmark directory to path\n",
        "benchmark_dir = Path.cwd().parent if (Path.cwd().parent / 'benchmark_utils').exists() else Path.cwd()\n",
        "sys.path.insert(0, str(benchmark_dir))\n",
        "\n",
        "from benchmark_utils import (\n",
        "    warmup,\n",
        "    clear_all_caches,\n",
        "    benchmark_with_cuda_events,\n",
        "    compute_statistics,\n",
        "    format_statistics,\n",
        "    BenchmarkVisualizer,\n",
        ")\n",
        "\n",
        "print(\"✓ Benchmark utilities loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Basic GPU Operation Benchmarking\n",
        "\n",
        "Let's benchmark a simple matrix multiplication operation with proper warmup and timing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_matmul():\n",
        "    \"\"\"Simple matrix multiplication on GPU.\"\"\"\n",
        "    x = torch.randn(1000, 1000, device='cuda')\n",
        "    return (x @ x.T).sum()\n",
        "\n",
        "# Benchmark with CUDA events\n",
        "print(\"Benchmarking matrix multiplication (1000x1000)...\")\n",
        "timings = benchmark_with_cuda_events(\n",
        "    simple_matmul,\n",
        "    warmup_iters=5,\n",
        "    repeat=20\n",
        ")\n",
        "\n",
        "# Compute and display statistics\n",
        "stats = compute_statistics(timings)\n",
        "print(\"\\nResults:\")\n",
        "print(format_statistics(stats))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Comparing Multiple Operations\n",
        "\n",
        "Let's compare different matrix sizes to see scaling behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sizes = [512, 1024, 2048]\n",
        "results = {}\n",
        "results_raw = {}\n",
        "\n",
        "for size in sizes:\n",
        "    def matmul_op():\n",
        "        x = torch.randn(size, size, device='cuda')\n",
        "        return (x @ x.T).sum()\n",
        "    \n",
        "    print(f\"\\nBenchmarking {size}x{size} matrix...\")\n",
        "    timings = benchmark_with_cuda_events(matmul_op, warmup_iters=3, repeat=15)\n",
        "    \n",
        "    name = f\"Size {size}x{size}\"\n",
        "    results[name] = compute_statistics(timings)\n",
        "    results_raw[name] = timings\n",
        "    \n",
        "    print(f\"  Mean: {results[name]['mean']:.2f} ms ± {results[name]['std']:.2f} ms\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Summary\")\n",
        "print(\"=\"*70)\n",
        "for name in results:\n",
        "    s = results[name]\n",
        "    print(f\"{name}: {s['mean']:.2f} ms (P95: {s['p95']:.2f} ms, CV: {s['cv']*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Creating Visualizations\n",
        "\n",
        "Now let's create publication-ready plots from our benchmark results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizer\n",
        "visualizer = BenchmarkVisualizer()\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path('./tutorial_results')\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Generate all plots\n",
        "visualizer.create_all_plots(\n",
        "    results=results,\n",
        "    results_raw=results_raw,\n",
        "    output_dir=output_dir,\n",
        "    prefix='matmul_comparison'\n",
        ")\n",
        "\n",
        "print(f\"\\nPlots saved to {output_dir}/\")\n",
        "print(\"Generated files:\")\n",
        "for f in sorted(output_dir.glob('matmul_comparison_*')):\n",
        "    print(f\"  - {f.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display the generated plots\n",
        "\n",
        "Let's view the visualizations we created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "# Display bar chart\n",
        "print(\"Bar Chart with Error Bars:\")\n",
        "display(Image(filename=str(output_dir / 'matmul_comparison_bars.png')))\n",
        "\n",
        "# Display box plot\n",
        "print(\"\\nBox Plot:\")\n",
        "display(Image(filename=str(output_dir / 'matmul_comparison_box.png')))\n",
        "\n",
        "# Display violin plot\n",
        "print(\"\\nViolin Plot:\")\n",
        "display(Image(filename=str(output_dir / 'matmul_comparison_violin.png')))\n",
        "\n",
        "# Display markdown table\n",
        "print(\"\\nStatistics Table:\")\n",
        "with open(output_dir / 'matmul_comparison_table.md', 'r') as f:\n",
        "    display(Markdown(f.read()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Running E2E Benchmark in Statistical Mode\n",
        "\n",
        "The E2E benchmark measures end-to-end latency from disk to GPU. Let's run it with statistical mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run E2E benchmark in statistical mode\n",
        "# Note: This requires benchmark data files to be present\n",
        "\n",
        "!cd .. && python benchmark_e2e.py \\\n",
        "    --statistical \\\n",
        "    --warmup 3 \\\n",
        "    --repeat 10 \\\n",
        "    --qubits 16 \\\n",
        "    --samples 100 \\\n",
        "    --frameworks mahout-parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Running with Visualization\n",
        "\n",
        "Now let's run the same benchmark but generate publication-ready plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run with visualization enabled\n",
        "!cd .. && python benchmark_e2e.py \\\n",
        "    --statistical \\\n",
        "    --visualize \\\n",
        "    --warmup 3 \\\n",
        "    --repeat 10 \\\n",
        "    --qubits 16 \\\n",
        "    --samples 100 \\\n",
        "    --frameworks mahout-parquet \\\n",
        "    --output-dir ./tutorial_e2e_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the generated E2E plots\n",
        "e2e_dir = Path('../tutorial_e2e_results')\n",
        "if e2e_dir.exists():\n",
        "    print(\"Generated E2E visualization files:\")\n",
        "    for f in sorted(e2e_dir.glob('*.png')):\n",
        "        print(f\"\\n{f.name}:\")\n",
        "        display(Image(filename=str(f)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Best Practices for Reproducible Benchmarks\n",
        "\n",
        "### 1. Always Use Warmup\n",
        "\n",
        "Warmup iterations eliminate JIT compilation overhead and stabilize measurements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BAD: No warmup - first run includes JIT overhead\n",
        "def benchmark_no_warmup():\n",
        "    timings = []\n",
        "    for i in range(10):\n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        end = torch.cuda.Event(enable_timing=True)\n",
        "        start.record()\n",
        "        simple_matmul()\n",
        "        end.record()\n",
        "        torch.cuda.synchronize()\n",
        "        timings.append(start.elapsed_time(end))\n",
        "    return timings\n",
        "\n",
        "# GOOD: With warmup\n",
        "timings_with_warmup = benchmark_with_cuda_events(\n",
        "    simple_matmul,\n",
        "    warmup_iters=5,\n",
        "    repeat=10\n",
        ")\n",
        "\n",
        "print(\"Without proper warmup: First run may be 2-10x slower!\")\n",
        "print(\"Always use warmup for fair benchmarks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Clear Caches Between Frameworks\n",
        "\n",
        "When comparing frameworks, clear caches to ensure fair comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark Framework A\n",
        "clear_all_caches()\n",
        "timings_a = benchmark_with_cuda_events(simple_matmul, warmup_iters=3, repeat=10)\n",
        "\n",
        "# IMPORTANT: Clear cache before next framework\n",
        "clear_all_caches()\n",
        "\n",
        "# Benchmark Framework B\n",
        "timings_b = benchmark_with_cuda_events(simple_matmul, warmup_iters=3, repeat=10)\n",
        "\n",
        "print(\"✓ Caches cleared between framework benchmarks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Report Full Distributions, Not Just Means\n",
        "\n",
        "Mean alone can be misleading. Always report percentiles and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timings = benchmark_with_cuda_events(simple_matmul, warmup_iters=3, repeat=20)\n",
        "stats = compute_statistics(timings)\n",
        "\n",
        "# BAD: Only reporting mean\n",
        "print(f\"BAD:  Mean time: {stats['mean']:.2f} ms\")\n",
        "\n",
        "# GOOD: Reporting full distribution\n",
        "print(f\"\\nGOOD: Mean: {stats['mean']:.2f} ms ± {stats['std']:.2f} ms\")\n",
        "print(f\"      Median (P50): {stats['median']:.2f} ms\")\n",
        "print(f\"      P95: {stats['p95']:.2f} ms, P99: {stats['p99']:.2f} ms\")\n",
        "print(f\"      Range: [{stats['min']:.2f}, {stats['max']:.2f}] ms\")\n",
        "print(f\"      CV: {stats['cv']*100:.2f}% (lower is better)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Use Sufficient Repetitions\n",
        "\n",
        "More repetitions give more reliable statistics. Aim for:\n",
        "- Fast operations (< 1ms): 100+ repetitions\n",
        "- Medium operations (1-100ms): 20-50 repetitions\n",
        "- Slow operations (> 100ms): 10-20 repetitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different repetition counts\n",
        "for n_repeat in [5, 10, 20, 50]:\n",
        "    timings = benchmark_with_cuda_events(simple_matmul, warmup_iters=3, repeat=n_repeat)\n",
        "    stats = compute_statistics(timings)\n",
        "    print(f\"N={n_repeat:3d}: Mean={stats['mean']:.2f}ms, Std={stats['std']:.2f}ms, CV={stats['cv']*100:.2f}%\")\n",
        "\n",
        "print(\"\\nNotice: More repetitions → more stable statistics (lower CV)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Save Configuration for Reproducibility\n",
        "\n",
        "Always document your benchmark configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from benchmark_utils.config import BenchmarkConfig\n",
        "\n",
        "# Create configuration\n",
        "config = BenchmarkConfig.default()\n",
        "config.fairness.warmup_iters = 5\n",
        "config.fairness.repeat_runs = 20\n",
        "config.visualization.output_dir = \"./my_results\"\n",
        "\n",
        "# Save for reproducibility\n",
        "config.to_yaml('./my_benchmark_config.yaml')\n",
        "\n",
        "print(\"Configuration saved. Others can reproduce your benchmarks with:\")\n",
        "print(\"  config = BenchmarkConfig.from_yaml('my_benchmark_config.yaml')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This tutorial covered:\n",
        "\n",
        "1. ✅ Using benchmark_utils for direct benchmarking\n",
        "2. ✅ Creating publication-ready visualizations\n",
        "3. ✅ Running statistical benchmarks from command line\n",
        "4. ✅ Best practices for reproducible benchmarks:\n",
        "   - Always use warmup\n",
        "   - Clear caches between frameworks\n",
        "   - Report full distributions\n",
        "   - Use sufficient repetitions\n",
        "   - Save configuration\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Run full E2E benchmarks with `--statistical --visualize`\n",
        "- Compare your quantum framework against baselines\n",
        "- Use the generated plots in papers and blog posts\n",
        "- Share your benchmark configurations for reproducibility\n",
        "\n",
        "For more information, see:\n",
        "- [Benchmark Roadmap RFC](../../docs/BENCHMARK_ROADMAP.md)\n",
        "- [Benchmark Utils API Documentation](../benchmark_utils/README.md)\n",
        "- [Main Benchmark README](../README.md)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
